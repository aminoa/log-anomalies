{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Log Anomaly Detection Using LogAI for an Video Editing App with online AI Capabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-21T05:20:09.588378Z",
     "start_time": "2023-12-21T05:19:58.924218Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreszarta/venv/lib/python3.9/site-packages/logai/dataloader/data_loader.py:153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected[constants.LOG_TIMESTAMPS] = pd.to_datetime(\n"
     ]
    },
    {
     "data": {
      "text/plain": "             logline                  timestamp     User              Service\n0    Generic Details 2023-12-20 23:33:25.437531  user101      Scene Detection\n1    Generic Details 2023-12-20 23:33:30.437531  user101    Video Enhancement\n2    Generic Details 2023-12-20 23:33:32.437531  user102      Scene Detection\n3    Generic Details 2023-12-20 23:33:33.437531  user105      Scene Detection\n4    Generic Details 2023-12-20 23:33:37.437531  user103     Color Correction\n..               ...                        ...      ...                  ...\n495  Generic Details 2023-12-20 23:57:35.437531  user103  Subtitle Generation\n496  Generic Details 2023-12-20 23:57:38.437531  user102  Subtitle Generation\n497  Generic Details 2023-12-20 23:57:42.437531  user101  Subtitle Generation\n498  Generic Details 2023-12-20 23:57:44.437531  user101  Subtitle Generation\n499  Generic Details 2023-12-20 23:57:48.437531  user101    Video Enhancement\n\n[500 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>logline</th>\n      <th>timestamp</th>\n      <th>User</th>\n      <th>Service</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:33:25.437531</td>\n      <td>user101</td>\n      <td>Scene Detection</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:33:30.437531</td>\n      <td>user101</td>\n      <td>Video Enhancement</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:33:32.437531</td>\n      <td>user102</td>\n      <td>Scene Detection</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:33:33.437531</td>\n      <td>user105</td>\n      <td>Scene Detection</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:33:37.437531</td>\n      <td>user103</td>\n      <td>Color Correction</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:57:35.437531</td>\n      <td>user103</td>\n      <td>Subtitle Generation</td>\n    </tr>\n    <tr>\n      <th>496</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:57:38.437531</td>\n      <td>user102</td>\n      <td>Subtitle Generation</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:57:42.437531</td>\n      <td>user101</td>\n      <td>Subtitle Generation</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:57:44.437531</td>\n      <td>user101</td>\n      <td>Subtitle Generation</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>Generic Details</td>\n      <td>2023-12-20 23:57:48.437531</td>\n      <td>user101</td>\n      <td>Video Enhancement</td>\n    </tr>\n  </tbody>\n</table>\n<p>500 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from logai.dataloader.openset_data_loader import OpenSetDataLoader, OpenSetDataLoaderConfig\n",
    "\n",
    "#File Configuration\n",
    "filepath = os.path.join(\"..\", \"datasets\", \"video_editing_app_log_anomalous.log\") # Update the file path to the game log\n",
    "\n",
    "dataset_name = \"video_editing\"\n",
    "data_loader = OpenSetDataLoader(\n",
    "    OpenSetDataLoaderConfig(\n",
    "        dataset_name=dataset_name,\n",
    "        filepath=filepath)\n",
    ")\n",
    "\n",
    "logrecord = data_loader.load_data()\n",
    "\n",
    "logrecord.to_dataframe().head(500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocess\n",
    "\n",
    "In preprocessing step user can retrieve and replace any regex strings and clean the raw loglines. This\n",
    "can be very useful to improve information extraction of the unstructured part of logs,\n",
    "as well as generate more structured attributes with domain knowledge.\n",
    "\n",
    "Unnecessary for our specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T05:20:09.592843Z",
     "start_time": "2023-12-21T05:20:09.589594Z"
    }
   },
   "outputs": [],
   "source": [
    "from logai.preprocess.preprocessor import PreprocessorConfig, Preprocessor\n",
    "from logai.utils import constants\n",
    "\n",
    "loglines = logrecord.body[constants.LOGLINE_NAME]\n",
    "attributes = logrecord.attributes\n",
    "\n",
    "preprocessor_config = PreprocessorConfig(\n",
    "    custom_replace_list=[\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = Preprocessor(preprocessor_config)\n",
    "\n",
    "clean_logs, custom_patterns = preprocessor.clean_log(\n",
    "    loglines\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Parsing\n",
    "\n",
    "After preprocessing, we call auto-parsing algorithms to automatically parse the cleaned logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-21T05:21:31.484938Z",
     "start_time": "2023-12-21T05:20:09.593755Z"
    }
   },
   "outputs": [],
   "source": [
    "from logai.information_extraction.log_parser import LogParser, LogParserConfig\n",
    "from logai.algorithms.parsing_algo.drain import DrainParams\n",
    "\n",
    "# parsing\n",
    "parsing_algo_params = DrainParams(\n",
    "    sim_th=0.5, depth=5\n",
    ")\n",
    "\n",
    "log_parser_config = LogParserConfig(\n",
    "    parsing_algorithm=\"drain\",\n",
    "    parsing_algo_params=parsing_algo_params\n",
    ")\n",
    "\n",
    "parser = LogParser(log_parser_config)\n",
    "parsed_result = parser.parse(clean_logs)\n",
    "\n",
    "parsed_loglines = parsed_result['parsed_logline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Time-series Anomaly Detection\n",
    "\n",
    "Here we show an example to conduct time-series anomaly detection with parsed logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "After parsing the logs and get log templates, we can extract timeseries features by coverting\n",
    "these parsed loglines into counter vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-21T05:21:35.953719Z",
     "start_time": "2023-12-21T05:21:31.485253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416574\n"
     ]
    }
   ],
   "source": [
    "from logai.information_extraction.feature_extractor import FeatureExtractorConfig, FeatureExtractor\n",
    "\n",
    "config = FeatureExtractorConfig(\n",
    "    group_by_time=\"15 min\",\n",
    "    group_by_category=['parsed_logline', 'Service', 'User'],\n",
    ")\n",
    "\n",
    "feature_extractor = FeatureExtractor(config)\n",
    "\n",
    "timestamps = logrecord.timestamp['timestamp']\n",
    "parsed_loglines = parsed_result['parsed_logline']\n",
    "counter_vector = feature_extractor.convert_to_counter_vector(\n",
    "    log_pattern=parsed_loglines,\n",
    "    attributes=attributes,\n",
    "    timestamps=timestamps\n",
    ")\n",
    "\n",
    "print(len(counter_vector))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "With the generated `counter_vector`, you can use `AnomalyDetector` to detect timeseries anomalies.\n",
    "Here we use an algorithm in Merlion library called `DynamicBaseLine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-12-21T05:22:01.545250Z",
     "start_time": "2023-12-21T05:21:35.955959Z"
    }
   },
   "outputs": [],
   "source": [
    "from logai.algorithms.anomaly_detection_algo.dbl import DBLDetectorParams\n",
    "from logai.analysis.anomaly_detector import AnomalyDetector, AnomalyDetectionConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "counter_vector[\"attribute\"] = counter_vector.drop(\n",
    "                [\n",
    "                    constants.LOG_COUNTS,\n",
    "                    constants.LOG_TIMESTAMPS,\n",
    "                    constants.EVENT_INDEX\n",
    "                ],\n",
    "                axis=1\n",
    "            ).apply(\n",
    "                lambda x: \"-\".join(x.astype(str)), axis=1\n",
    "            )\n",
    "\n",
    "attr_list = counter_vector[\"attribute\"].unique()\n",
    "\n",
    "\n",
    "params = DBLDetectorParams(\n",
    ")\n",
    "\n",
    "\n",
    "anomaly_detection_config = AnomalyDetectionConfig(\n",
    "    algo_name='dbl',\n",
    "    algo_params=params\n",
    ")\n",
    "\n",
    "res = pd.DataFrame()\n",
    "for attr in attr_list:\n",
    "    temp_df = counter_vector[counter_vector[\"attribute\"] == attr]\n",
    "    if temp_df.shape[0] >= constants.MIN_TS_LENGTH:\n",
    "        train, test = train_test_split(\n",
    "            temp_df[[constants.LOG_TIMESTAMPS, constants.LOG_COUNTS]],\n",
    "            shuffle=False,\n",
    "            train_size=0.5,\n",
    "        )\n",
    "        anomaly_detector = AnomalyDetector(anomaly_detection_config)\n",
    "        anomaly_detector.fit(train)\n",
    "        anom_score = anomaly_detector.predict(test)\n",
    "        res = res._append(anom_score)\n",
    "        \n",
    "anomaly_threshold = 5.0\n",
    "filtered_res = res[res['anom_score'] >= anomaly_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T05:22:01.551889Z",
     "start_time": "2023-12-21T05:22:01.548373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": "        parsed_logline     Service     User           timestamp  \\\n14163  Generic Details  Audio Sync  user101 2024-05-16 12:15:00   \n\n                                             event_index  counts  \\\n14163  [4249905, 4249909, 4249953, 4249965, 4249977, ...    5010   \n\n                                attribute  \n14163  Generic Details-Audio Sync-user101  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>parsed_logline</th>\n      <th>Service</th>\n      <th>User</th>\n      <th>timestamp</th>\n      <th>event_index</th>\n      <th>counts</th>\n      <th>attribute</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14163</th>\n      <td>Generic Details</td>\n      <td>Audio Sync</td>\n      <td>user101</td>\n      <td>2024-05-16 12:15:00</td>\n      <td>[4249905, 4249909, 4249953, 4249965, 4249977, ...</td>\n      <td>5010</td>\n      <td>Generic Details-Audio Sync-user101</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get anomalous datapoints\n",
    "anomalies = counter_vector.iloc[filtered_res[filtered_res>0].index]\n",
    "print(len(anomalies))\n",
    "anomalies.head(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
